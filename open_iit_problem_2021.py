# -*- coding: utf-8 -*-
"""Open IIT problem 2021.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jkBFcQ9l59aeyTeWEqg013XavBjs_A7I
"""

import numpy as np
import pandas as pd

Data = pd.read_csv('Train_data.csv')

Data

import seaborn as sns
#plt.figure(figsize=(15,10))
sns.heatmap(Data.corr(), annot=True,linewidths=0.4)

def get_data_corr(Data):
  corr_col = set()
  corrmat = Data.corr()
  for i in range(len(corrmat.columns)):
    for j in range(i):
      if abs(corrmat.iloc[i,j]) > 0.6:
        colname = corrmat.columns[i]
        corr_col.add(colname)
  return corr_col

corr_features = get_data_corr(Data)
corr_features

Data.isna().sum()

def date_extract(column):
  col = np.zeros(column.shape)
  for i in range(len(col)):
    date = int(column[i][0:2])
    col[i] = date
    #date = int(column[i][0:2])
  return col

def month_extract(column):
  col = np.zeros(column.shape)
  for i in range(len(column)):
    month = int(column[i][3:5])
    col[i] = month
    #date = int(column[i][0:2])
  return col

def AFeature(Data):
  Data.insert(loc=16, column='AFeature', value=0)
  for i in range(len(Data)):

    if Data.iloc[i,1] < 0 and Data.iloc[i,2] < 0:
      Data.iloc[i,16] = -(Data.iloc[i,1] * Data.iloc[i,2])
    if Data.iloc[i,1] < 0 and Data.iloc[i,2] > 0:
      Data.iloc[i,16] = Data.iloc[i,1] + Data.iloc[i,2]
    if Data.iloc[i,1] < 0 and Data.iloc[i,2] < 0:
      Data.iloc[i,16] = Data.iloc[i,1] + Data.iloc[i,2]
    else:
      Data.iloc[i,16] = Data.iloc[i,1] * Data.iloc[i,2]
  return Data

## id, release_date, year to be removed
from sklearn.preprocessing import LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder

Data = Data.drop(columns='id', axis=1)
Data.insert(loc=15, column='Date', value=0)
Data['Date'] = date_extract(Data['release_date'])
# Data.insert(loc=16, column='AFeature', value=0)
# Data['AFeature'] = Data['loudness']*Data['energy']
Data = AFeature(Data)
#Data['instrumentalness'] = np.log2(Data['instrumentalness']+0.000001)
Data['year'] -= 2000
# Data['year_squared'] = Data['year']**2
# Data['year_cubed'] = Data['year']**3
#Data['duration-min'] *= 60
#Data.insert(loc=15, column='year_sq', value=0)
#Data['year_sq'] = Data['year']**2
# Data['Date'] = date_extract(Data['release_date'])#Data['year']**3
#Data['AFeature'] = Data['year']*Data['energy']*Data['danceability']
Data.insert(loc=16, column='month', value=0)
Data['month'] = month_extract(Data['release_date'])
#Data.insert(loc=16, column='year_cube', value=0)
#Data['year_cube'] = Data['year']**3
le = LabelEncoder()
Data['explicit'] = le.fit_transform(Data['explicit'])
Data['mode'] = le.fit_transform(Data['mode'])
#Data['popularity'] = le.fit_transform(Data['popularity'])

Data.drop('release_date', axis=1, inplace=True)
#Data.insert(loc=16, column='AFeature', value=0)
#Data['AFeature'] = Data['danceability']/(Data['acousticness']*Data['liveness'])
#Data['AFeature'] = Data['energy']/(Data['explicit']+0.0001)
#Data['BFeature'] = Data['year']**3
#ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [-1])], remainder='passthrough')
#Data = pd.DataFrame(ct.fit_transform(Data))

for i in range(len(Data)):
  val = Data.iloc[i,-1]
  if val == 'very low':
    Data.iloc[i,-1] = 0
  elif val == 'low':
    Data.iloc[i,-1] = 1
  elif val == 'average':
    Data.iloc[i,-1] = 2
  elif val == 'high':
    Data.iloc[i,-1] = 3
  else:
    Data.iloc[i,-1] = 4

len(Data)

from pandas_profiling import ProfileReport
profile = ProfileReport(df=Data)
profile.to_file(output_file='data.html')

predictions_csv = Data.to_csv('Train_OpenIIT.csv', index = True)

ind = []
for i in range(len(Data)):
  if Data.iloc[i, 13] > 15: #or Data.iloc[i, 7] < -29.7:
    ind.append(i)
Data.drop(labels=ind, inplace=True)

Data.rename({'release_date':'release_month'}, inplace=True)

Data['key'].value_counts().sort_values(ascending=False)

Data

Data['popularity'].unique()

from sklearn.model_selection import train_test_split
X = Data.iloc[:, :-1].values
y = Data.iloc[:, -1].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.328) # size of y_test ~ 4000

y_test.shape

from imblearn.over_sampling import SMOTE
sm = SMOTE()
# X = Data.iloc[:, :-1].values
# y = Data.iloc[:, -1].values
training_inputs, training_outputs_labels = sm.fit_resample(X, y)
X_train, X_eval, y_train, y_eval = train_test_split(training_inputs, training_outputs_labels, test_size=0.30, random_state=1, shuffle=True)

y_train = y_train.astype('int')
y_test = y_test.astype('int')

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.fit_transform(X_test)

print(X_train)

X_train.shape

y_test.shape

from sklearn import tree
treeclf = tree.DecisionTreeClassifier()
treeclf.fit(X_train,y_train)

from sklearn.ensemble import RandomForestClassifier
ranclf = RandomForestClassifier()
ranclf.fit(X_train, y_train)

from sklearn.metrics import accuracy_score, confusion_matrix
ran_pred = ranclf.predict(X_test)
acc = accuracy_score(y_test, ran_pred)
#cm = confusion_matrix(y_test, y_pred)
print("RandomForest's accuracy: "+str(acc))
#print(cm)

import matplotlib.pyplot as plt
#plt.plot(ranclf.feature_importances_)
#plt.xticks(np.arange(X_train.shape[1]), X_train.columns.toList(), rotation=90)
Data2 = Data.drop('popularity', axis=1)
features = pd.Series(ranclf.feature_importances_, Data2.columns).sort_values(ascending = False)
features.plot(kind = 'bar')

#from sklearn.metrics import accuracy_score, confusion_matrix
y_pred = treeclf.predict(X_test)
acc = accuracy_score(y_test, y_pred)
#cm = confusion_matrix(y_test, y_pred)
print("Tree's accuracy: "+str(acc))
#print(cm)

X_test_2 = X_train[5000:9000]
X_test_2.add(X_test[:200])
y_test_2 =

import xgboost as xgb
xgbclf = xgb.XGBClassifier(learning_rate=0.09, n_estimators=100)
xgbclf.fit(X_train, y_train)

from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import RandomizedSearchCV
from xgboost.sklearn import XGBClassifier
import xgboost as xgb
import tqdm

xgb1 = XGBClassifier(learning_rate =0.1,
 n_estimators=500,
 max_depth=5,
 min_child_weight=1,
 gamma=0,
 subsample=0.8,
 colsample_bytree=0.8,
 objective= 'binary:softmax',
 nthread=4,
 scale_pos_weight=1,
 seed=27)
xgb1.fit(X_train, y_train)

xgb1.score(X_test, y_test)

y_pred = xgbclf.predict(X_test)
acc = accuracy_score(y_test, y_pred)
#cm = confusion_matrix(y_test, y_pred)
print("XGB's accuracy: "+str(acc))
#print(cm)

from sklearn.ensemble import BaggingClassifier
bagclf = BaggingClassifier(tree.DecisionTreeClassifier(random_state=1))
bagclf.fit(X_train, y_train)

bagclf.score(X_test, y_test)

from sklearn.ensemble import GradientBoostingClassifier
gbc= GradientBoostingClassifier(learning_rate=0.25,random_state=1)
gbc.fit(X_train, y_train)

gbc.score(X_test, y_test)

from sklearn import svm
from sklearn.model_selection import GridSearchCV
svc = svm.SVC()
parameters = {'kernel':('linear', 'rbf'), 'C':[1000]}
gsclf = GridSearchCV(svc, parameters)
gsclf.fit(X_train, y_train)

y_pred = gsclf.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print("GridSearch's accuracy: "+str(acc))

# make a prediction with a stacking ensemble
# from sklearn.datasets import make_classification
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
# from sklearn.neighbors import KNeighborsClassifier
# from sklearn.tree import DecisionTreeClassifier
# from sklearn.svm import SVC
# from sklearn.naive_bayes import GaussianNB

level0 = list()
level0.append(('tree',treeclf))
level0.append(('ran',ranclf))
level0.append(('xgb',xgbclf))
level0.append(('gbc',gbc))
# define meta learner model
level1 = LogisticRegression() #xgb.XGBClassifier()
# define the stacking ensemble
model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)
# fit the model on all available data
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(acc)

import statistics
def find_max_mode(list1):
    list_table = statistics._counts(list1)
    len_table = len(list_table)

    if len_table == 1:
        max_mode = statistics.mode(list1)
    else:
        new_list = []
        for i in range(len_table):
            new_list.append(list_table[i][0])
        max_mode = max(new_list) # use the max value here
    return max_mode

def find_ceil(list2):
  return max(list2)

#import statistics
import math
pred1=ranclf.predict(X_test)
pred2=xgbclf.predict(X_test)
pred3=bagclf.predict(X_test)

final_pred = np.array([])
for i in range(0,len(X_test)):
    final_pred = np.append(final_pred, find_ceil([pred1[i], pred2[i], pred3[i]]))

acc = accuracy_score(y_test, final_pred)
print(acc)

print(len(y_test))

r=0
b=0
xgb_pred = xgbclf.predict(X_test)
for i in range(len(y_test)):
  if xgb_pred[i] >= y_test[i]:
    b += xgb_pred[i]
    r += 2*y_test[i]

print('Revenue: '+str(r))
print('bidding: '+str(b))
print('max bidding: '+str(len(y_test)*2.5))

from numpy import hstack
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB

 
# get a list of base models
def get_models():
	models = list()
	models.append(('lr', LogisticRegression()))
	models.append(('knn', KNeighborsClassifier()))
	models.append(('cart', DecisionTreeClassifier()))
	models.append(('svm', SVC()))
	models.append(('bayes', GaussianNB()))
	return models
 
# fit the blending ensemble
def fit_ensemble(models, X_train, X_val, y_train, y_val):
	# fit all models on the training set and predict on hold out set
	meta_X = list()
	for name, model in models:
		# fit in training set
		model.fit(X_train, y_train)
		# predict on hold out set
		yhat = model.predict(X_val)
		# reshape predictions into a matrix with one column
		yhat = yhat.reshape(len(yhat), 1)
		# store predictions as input for blending
		meta_X.append(yhat)
	# create 2d array from predictions, each set is an input feature
	meta_X = hstack(meta_X)
	# define blending model
	blender = LogisticRegression()
	# fit on predictions from base models
	blender.fit(meta_X, y_val)
	return blender
 
# make a prediction with the blending ensemble
def predict_ensemble(models, blender, X_test):
	# make predictions with base models
	meta_X = list()
	for name, model in models:
		# predict with base model
		yhat = model.predict(X_test)
		# reshape predictions into a matrix with one column
		yhat = yhat.reshape(len(yhat), 1)
		# store prediction
		meta_X.append(yhat)
	# create 2d array from predictions, each set is an input feature
	meta_X = hstack(meta_X)
	# predict
	return blender.predict(meta_X)
 
# create the base models
models = get_models()
# train the blending ensemble
blender = fit_ensemble(models, X_train, X_eval, y_train, y_eval)

blend_pred = predict_ensemble(models, blender, X_test)
acc = accuracy_score(y_test, blend_pred)
print(acc)

pseudo = []
for i in range()



from sklearn.ensemble import VotingClassifier
model = VotingClassifier(estimators=[(ranclf), (xgbclf), (gbc)], voting='hard')
model.fit(X_train, y_train)

print(y_pred[100:110])

#import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense

model = Sequential()
model.add(Dense(input_dim=13, units=16, activation='relu'))
model.add(Dense(units=18, activation='relu'))
model.add(Dense(units=18, activation='relu'))
model.add(Dense(units=16, activation='relu'))
model.add(Dense(units=6, activation='relu'))
model.add(Dense(units=5, activation='sigmoid'))

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model.fit(X_train, y_train, batch_size=32, epochs=100)

def convert(y_pred):
  for i in range(len(y_pred)):
    maximum = max(y_pred[i])
    for j in range(5):
        if y_pred[i,j] == maximum:
          y_pred[i,j] = 1
        else:
          y_pred[i,j] = 0

y_pred = model.predict(X_test)
convert(y_pred)
from sklearn.metrics import accuracy_score, confusion_matrix
acc = accuracy_score(y_test, y_pred)
#cm = confusion_matrix(y_test, y_pred)
print(acc)
#print(cm)

print(y_pred[100:110])

print(y_test[100:110])